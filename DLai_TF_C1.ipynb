{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLai-TF-C1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/real3stone/ML-Ng/blob/master/DLai_TF_C1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L2N7Mm5rPKt",
        "colab_type": "text"
      },
      "source": [
        "## WEEK-1：模拟一次函数y=2x-1【单层神经网络】\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjJWOpe8qZVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')  # # tf.train.AdamOptimizer()\n",
        "\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500)\n",
        "print(model.predict([10.0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_aoVaSs6I4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## WEEK-2：Handwriting Recognition-[DNN]\n",
        "数据集可以直接从keras中获取；程序中加入callback()，监测训练情况\n",
        "\n",
        "**Dataset**：MNIST（图片全部为(28,28)的灰图）\n",
        "图片全部非常规整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bku0BN51ryGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.99):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),  # 'adam'\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpyRzrvA5iV7",
        "colab_type": "text"
      },
      "source": [
        "## WEEK-3：Handwriting Recognition -[CNN]\n",
        "用CNN网络来提升week2-ex的性能"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULq77kK_1ein",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.998):\n",
        "      print(\"\\nReached 99.8% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8YnS9KQLJvH",
        "colab_type": "text"
      },
      "source": [
        "## WEEK-4：Fit-generator\n",
        "用imagegenerator来直接从目录产生label，之后训练时，从原来的model.fit()变为model.fit_generator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O42bG0y-2kDo",
        "colab_type": "text"
      },
      "source": [
        "**EX1：人马分类器**\n",
        "\n",
        "分别用两个generator()产生train和validation的label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJUUijrd15ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoLmzXDh2-QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()\n",
        "\n",
        "# Directory with our training horse/human pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our validation horse/human pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiZPHENf3bcH",
        "colab_type": "text"
      },
      "source": [
        "展示部分图片"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpoKMCMv3WV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0\n",
        "\n",
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "validation_horse_hames = os.listdir(validation_horse_dir)\n",
        "validation_human_names = os.listdir(validation_human_dir)\n",
        "\n",
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQeSkcPz3-ko",
        "colab_type": "text"
      },
      "source": [
        "**Building a Small Model from Scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agcTxbQ54BKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXyLSUWu4Sq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIiWhI0u4Vtv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "36ca65ea-b41d-4da5-fe7a-69d2ba0541e4"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5cLAUNz4biA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "91a77250-7ebd-4c1b-9811-ccaec4268351"
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "8/8 [==============================] - 90s 11s/step - loss: 117.8496 - acc: 0.5295 - val_loss: 1.2973 - val_acc: 0.5039\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 1.0131 - acc: 0.5317 - val_loss: 1.8231 - val_acc: 0.5195\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.7596 - acc: 0.6251 - val_loss: 0.3870 - val_acc: 0.8320\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 98s 12s/step - loss: 0.2337 - acc: 0.9131 - val_loss: 1.3107 - val_acc: 0.6484\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.2254 - acc: 0.8921 - val_loss: 0.3825 - val_acc: 0.8906\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.7817 - acc: 0.8521 - val_loss: 6.8665 - val_acc: 0.5703\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 88s 11s/step - loss: 3.0097 - acc: 0.5929 - val_loss: 0.2794 - val_acc: 0.8750\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.3789 - acc: 0.8465 - val_loss: 1.6985 - val_acc: 0.7539\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.2702 - acc: 0.9255 - val_loss: 0.8379 - val_acc: 0.8125\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 82s 10s/step - loss: 0.0621 - acc: 0.9878 - val_loss: 1.0551 - val_acc: 0.8594\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.0187 - acc: 0.9967 - val_loss: 1.2936 - val_acc: 0.8711\n",
            "Epoch 12/15\n",
            "8/8 [==============================] - 97s 12s/step - loss: 1.3595 - acc: 0.8193 - val_loss: 12.1317 - val_acc: 0.5000\n",
            "Epoch 13/15\n",
            "8/8 [==============================] - 77s 10s/step - loss: 1.2529 - acc: 0.9406 - val_loss: 0.9393 - val_acc: 0.8438\n",
            "Epoch 14/15\n",
            "8/8 [==============================] - 97s 12s/step - loss: 0.0846 - acc: 0.9707 - val_loss: 1.9742 - val_acc: 0.7891\n",
            "Epoch 15/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 1.3167 - acc: 0.8231 - val_loss: 1.9310 - val_acc: 0.8242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOhuZ7MU4ket",
        "colab_type": "text"
      },
      "source": [
        "测试Model训练的效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5bbFXcR4hDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp_P0HfQ14dp",
        "colab_type": "text"
      },
      "source": [
        "EX2：表情照片分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtnI52XYCyvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "DESIRED_ACCURACY = 0.999\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\" \\\n",
        "    -O \"/tmp/happy-or-sad.zip\"\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/tmp/happy-or-sad.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp/h-or-s\")\n",
        "zip_ref.close()\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>DESIRED_ACCURACY):\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3px4ATF1m23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OQFCqQw1tde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9337a127-006e-4401-a892-7c1d636b2b77"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        \"/tmp/h-or-s\",  \n",
        "        target_size=(150, 150), \n",
        "        batch_size=10,\n",
        "        class_mode='binary')\n",
        "\n",
        "# Expected output: 'Found 80 images belonging to 2 classes'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW6O3FuW1wjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "3ceb41b0-8a90-4a37-ae89-da10353666e3"
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=2,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      callbacks=[callbacks])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "2/2 [==============================] - 2s 818ms/step - loss: 848.4290 - acc: 0.7000\n",
            "Epoch 2/15\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 191.4927 - acc: 0.5500\n",
            "Epoch 3/15\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 15.0330 - acc: 0.7500\n",
            "Epoch 4/15\n",
            "2/2 [==============================] - 0s 250ms/step - loss: 36.0837 - acc: 0.6000\n",
            "Epoch 5/15\n",
            "2/2 [==============================] - 0s 243ms/step - loss: 10.7937 - acc: 0.6500\n",
            "Epoch 6/15\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 1.2100 - acc: 0.8500\n",
            "Epoch 7/15\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 7.6634 - acc: 0.4500\n",
            "Epoch 8/15\n",
            "2/2 [==============================] - 0s 241ms/step - loss: 3.8456 - acc: 0.3500\n",
            "Epoch 9/15\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 21.2454 - acc: 0.3500\n",
            "Epoch 10/15\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.6822 - acc: 0.9500\n",
            "Epoch 11/15\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0498 - acc: 1.0000\n",
            "Reached 99.9% accuracy so cancelling training!\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0258 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1627gVC71ziV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}